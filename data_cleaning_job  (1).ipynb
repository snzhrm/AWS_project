{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.8 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: c8c46827-5483-49a0-bfc9-c37dd418fc7c\nApplying the following default arguments:\n--glue_kernel_version 1.0.8\n--enable-glue-datacatalog true\nWaiting for session c8c46827-5483-49a0-bfc9-c37dd418fc7c to get into ready status...\nSession c8c46827-5483-49a0-bfc9-c37dd418fc7c has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df = glueContext.create_dynamic_frame.from_catalog(\n    database = \"project\",\n    table_name = \"reddit_data\"\n).toDF()\n\ndf.printSchema()\ndf.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- col0: string (nullable = true)\n |-- col1: string (nullable = true)\n |-- col2: string (nullable = true)\n\n+--------------------+--------------------+-------------------+\n|                col0|                col1|               col2|\n+--------------------+--------------------+-------------------+\n|               title|                body|       createad_utc|\n|Time to Shake Thi...|**Posting again i...|2025-03-08T14:47:17|\n|98.3% of ultrasou...|                    |2025-04-18T07:50:00|\n|This ‘College Pro...|Massive Blue is h...|2025-04-17T11:02:16|\n|What are some of ...|I've recently bee...|2025-04-17T15:37:28|\n+--------------------+--------------------+-------------------+\nonly showing top 5 rows\n\n/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import col\n\ndf = df.withColumnRenamed(\"col0\", \"title\") \\\n       .withColumnRenamed(\"col1\", \"body\") \\\n       .withColumnRenamed(\"col2\", \"created_utc\")\ndf = df.filter((col(\"title\") != \"title\") & (col(\"body\") != \"body\") & (col(\"created_utc\") != \"created_utc\"))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import concat_ws, regexp_replace, trim, split, col\nfrom pyspark.ml.feature import StopWordsRemover\n\n# 1. Объединяем title + body → full_text\ndf_cleaned = df.withColumn(\"full_text\", concat_ws(\" \", col(\"title\"), col(\"body\"))) \\\n    .filter(col(\"full_text\").isNotNull())\n\n# 2. Очищаем текст\ndf_cleaned = df_cleaned.withColumn(\"full_text\", col(\"full_text\").cast(\"string\")) \\\n    .withColumn(\"full_text\", regexp_replace(col(\"full_text\"), r\"http\\S+|www\\S+|https\\S+\", \"\")) \\\n    .withColumn(\"full_text\", regexp_replace(col(\"full_text\"), r\"[^a-zA-Z\\s]\", \"\")) \\\n    .withColumn(\"full_text\", regexp_replace(col(\"full_text\"), r\"\\s+\", \" \")) \\\n    .withColumn(\"full_text\", trim(col(\"full_text\")))\n\n\n# 4. Разбиваем текст на слова\ndf_cleaned = df_cleaned.withColumn(\"words\", split(col(\"full_text\"), \" \"))\n\n# 5. Удаляем стоп-слова\nremover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\ndf_cleaned = remover.transform(df_cleaned)\n\ndf_cleaned = df_cleaned.drop(\"title\", \"body\", \"words\")",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_cleaned.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+--------------------+\n|        created_utc|           full_text|      filtered_words|\n+-------------------+--------------------+--------------------+\n|2025-03-08T14:47:17|Time to Shake Thi...|[Time, Shake, Thi...|\n|2025-04-18T07:50:00|of ultrasound exa...|[ultrasound, exam...|\n|2025-04-17T11:02:16|This College Prot...|[College, Protest...|\n|2025-04-17T15:37:28|What are some of ...|[biggest, fears, ...|\n|2025-04-17T19:56:52|An AI bot just us...|[AI, bot, used, n...|\n|2025-04-17T20:48:28|what major should...|[major, choose, d...|\n|2025-04-18T04:41:37|OneMinute Daily A...|[OneMinute, Daily...|\n|2025-04-17T15:50:05|Just like ChatGPT...|[like, ChatGPT, G...|\n|2025-04-17T22:47:05|I had no idea how...|[idea, much, stuf...|\n|2025-04-18T03:42:55|What if AI was as...|[AI, advanced, po...|\n|2025-04-17T16:20:36|Court documents r...|[Court, documents...|\n|2025-04-16T21:54:30|Why nobody use AI...|[nobody, use, AI,...|\n|2025-04-16T18:13:52|Whats the most un...|[Whats, unexpecte...|\n|2025-04-18T02:03:45|Voting for the Mo...|[Voting, Intellig...|\n|2025-04-18T08:43:39|Is AI smarter tha...|[AI, smarter, yea...|\n|2025-04-18T00:14:57|Pitch for a video...|[Pitch, video, Iv...|\n|2025-04-17T10:27:09|Is this why LLM a...|[LLM, powerful, I...|\n|2025-04-17T05:10:59|Use of AI increas...|[Use, AI, increas...|\n|2025-04-17T03:02:19|Will AIsavvy empl...|[AIsavvy, employe...|\n|2025-04-17T15:21:18|The Role of AI in...|[Role, AI, Job, D...|\n+-------------------+--------------------+--------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "from awsglue.dynamicframe import DynamicFrame\n\n# Преобразуем DataFrame в DynamicFrame\ndyf_cleaned = DynamicFrame.fromDF(df_cleaned, glueContext, \"dyf_cleaned\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "glueContext.write_dynamic_frame.from_options(\n    frame=dyf_cleaned,\n    connection_type=\"s3\",\n    connection_options={\n        \"path\": \"s3://aws-nlp-project/cleaned_csv/\",\n        \"partitionKeys\": []  # Можно добавить ключи партиционирования, если нужно\n    },\n    format=\"parquet\",\n    format_options={\"compression\": \"snappy\"},\n    transformation_ctx=\"write_cleaned_reddit\"\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7fdb647f5010>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_cleaned.write.mode(\"overwrite\").option(\"header\", \"true\") \\\n    .csv(\"s3://aws-nlp-project/cleaned_csv/\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		}
	]
}